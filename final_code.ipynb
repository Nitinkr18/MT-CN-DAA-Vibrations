{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891721ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D, \n",
    "    Reshape, multiply, add, Activation, Lambda, Concatenate, Dropout\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "\n",
    "# --- Configuration ---\n",
    "DATASET_PATH = 'D:/challa' \n",
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.0005\n",
    "\n",
    "# --- 1. Data Loading ---\n",
    "def parse_filenames_to_dataframe(dataset_path):\n",
    "    records = []\n",
    "    expanded_path = os.path.expanduser(dataset_path)\n",
    "    print(f\"Scanning main directory: {expanded_path}...\")\n",
    "\n",
    "    if not os.path.isdir(expanded_path):\n",
    "        print(f\"Error: Directory not found. Check DATASET_PATH.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    bearing_dirs = [d for d in os.listdir(expanded_path) \n",
    "                    if os.path.isdir(os.path.join(expanded_path, d)) and not d.startswith('.')]\n",
    "    print(f\"Found bearing type folders: {bearing_dirs}\")\n",
    "    \n",
    "    for bearing_type in bearing_dirs:\n",
    "        bearing_path = os.path.join(expanded_path, bearing_type)\n",
    "        for root, _, files in os.walk(bearing_path):\n",
    "            for file in files:\n",
    "                if file.endswith('.mat'):\n",
    "                    file_parts = file.replace('.mat', '').split('_')\n",
    "                    if len(file_parts) == 5:\n",
    "                        speed = int(file_parts[4])\n",
    "                        records.append({\n",
    "                            'filepath': os.path.join(root, file),\n",
    "                            'filename': file,\n",
    "                            'speed': speed\n",
    "                        })\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    print(f\"Scan complete. Found {len(df)} total .mat files.\")\n",
    "    return df\n",
    "\n",
    "# --- 2. Label Processing ---\n",
    "def process_labels(df):\n",
    "    print(\"\\nProcessing labels for joint classification and domain adaptation...\")\n",
    "    \n",
    "    df['joint_code'] = df['filename'].apply(lambda f: f.split('_')[0] + '_' + f.split('_')[1])\n",
    "    df['joint_label'], joint_classes = pd.factorize(df['joint_code'])\n",
    "    \n",
    "    df['domain_label'], domain_classes = pd.factorize(df['speed'])\n",
    "    \n",
    "    print(\"Label processing complete.\")\n",
    "    print(f\"Found {len(joint_classes)} unique joint conditions.\")\n",
    "    print(f\"Found {len(domain_classes)} unique domains (speeds).\")\n",
    "    \n",
    "    return df, list(domain_classes), list(joint_classes)\n",
    "\n",
    "# --- 3. Data Pipeline ---\n",
    "def load_and_process_file(filepath, joint_label, domain_label):\n",
    "    mat_contents = scipy.io.loadmat(filepath.numpy())\n",
    "    spectrogram_data = mat_contents['Spectrogram'].astype(np.float32)\n",
    "    \n",
    "    if np.max(spectrogram_data) > 0:\n",
    "        spectrogram_data = spectrogram_data / np.max(spectrogram_data)\n",
    "    \n",
    "    images = np.expand_dims(spectrogram_data, axis=-1)\n",
    "    \n",
    "    num_samples = images.shape[0]\n",
    "    \n",
    "    joint_labels = to_categorical([joint_label] * num_samples, num_classes=32)\n",
    "    domain_labels = to_categorical([domain_label] * num_samples, num_classes=6)\n",
    "    \n",
    "    return images, joint_labels, domain_labels\n",
    "\n",
    "def tf_load_and_process(filepath, labels):\n",
    "    images, j_labels, d_labels = tf.py_function(\n",
    "        func=load_and_process_file,\n",
    "        inp=[filepath, labels['joint'], labels['domain']],\n",
    "        Tout=(tf.float32, tf.float32, tf.float32)\n",
    "    )\n",
    "    \n",
    "    images.set_shape([None, IMG_SIZE, IMG_SIZE, 1])\n",
    "    j_labels.set_shape([None, 32])\n",
    "    d_labels.set_shape([None, 6])\n",
    "    \n",
    "    labels_dict = {\n",
    "        \"joint_output\": j_labels,\n",
    "        \"domain_output\": d_labels\n",
    "    }\n",
    "    \n",
    "    return tf.data.Dataset.from_tensor_slices((images, labels_dict))\n",
    "\n",
    "# --- 4. Model Architecture ---\n",
    "@tf.custom_gradient\n",
    "def grad_reverse(x, lambda_val=1.0):\n",
    "    y = tf.identity(x)\n",
    "    def custom_grad(dy):\n",
    "        return -lambda_val * dy, None\n",
    "    return y, custom_grad\n",
    "\n",
    "class GradientReversal(tf.keras.layers.Layer):\n",
    "    def __init__(self, lambda_val=1.0):\n",
    "        super().__init__()\n",
    "        self.lambda_val = lambda_val\n",
    "\n",
    "    def call(self, x):\n",
    "        return grad_reverse(x, self.lambda_val)\n",
    "\n",
    "def cbam_module(x, ratio=8):\n",
    "    channels = x.shape[-1]\n",
    "    avg_pool = GlobalAveragePooling2D()(x)\n",
    "    max_pool = tf.keras.layers.GlobalMaxPooling2D()(x)\n",
    "    shared_dense_one = Dense(channels // ratio, activation='relu')\n",
    "    shared_dense_two = Dense(channels)\n",
    "    avg_out = shared_dense_two(shared_dense_one(avg_pool))\n",
    "    max_out = shared_dense_two(shared_dense_one(max_pool))\n",
    "    channel_attention = Activation('sigmoid')(add([avg_out, max_out]))\n",
    "    channel_attention = Reshape((1, 1, channels))(channel_attention)\n",
    "    x_channel = multiply([x, channel_attention])\n",
    "    avg_pool_spatial = Lambda(lambda y: tf.reduce_mean(y, axis=3, keepdims=True))(x_channel)\n",
    "    max_pool_spatial = Lambda(lambda y: tf.reduce_max(y, axis=3, keepdims=True))(x_channel)\n",
    "    concat = Concatenate(axis=3)([avg_pool_spatial, max_pool_spatial])\n",
    "    spatial_attention = Conv2D(1, (7, 7), padding='same', activation='sigmoid')(concat)\n",
    "    return multiply([x_channel, spatial_attention])\n",
    "\n",
    "def build_efficient_model(input_shape=(128, 128, 1)):\n",
    "    inputs = Input(shape=input_shape, name='input')\n",
    "    \n",
    "    # Add L2 regularization to the convolutional layers\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001))(inputs)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001))(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = cbam_module(x, ratio=8)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001))(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001))(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    features = Flatten()(x)\n",
    "    features = Dropout(0.5)(features)\n",
    "    \n",
    "    # Add L2 regularization to the dense layers\n",
    "    joint_head = Dense(256, activation='relu', kernel_regularizer=l2(0.001))(features)\n",
    "    joint_head = Dropout(0.3)(joint_head)\n",
    "    joint_output = Dense(32, activation='softmax', name='joint_output')(joint_head)\n",
    "    \n",
    "    domain_features = GradientReversal(lambda_val=0.5)(features)\n",
    "    # --- SUGGESTED ADDITION IS HERE ---\n",
    "    domain_head = Dense(128, activation='relu', kernel_regularizer=l2(0.001))(domain_features)\n",
    "    domain_output = Dense(6, activation='softmax', name='domain_output')(domain_head)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=[joint_output, domain_output])\n",
    "    return model\n",
    "\n",
    "# --- 5. Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    master_df = parse_filenames_to_dataframe(DATASET_PATH)\n",
    "    \n",
    "    if not master_df.empty:\n",
    "        master_df, domain_classes, joint_classes = process_labels(master_df)\n",
    "        \n",
    "        train_df, val_df = train_test_split(\n",
    "            master_df, test_size=0.2, random_state=42, stratify=master_df['joint_label']\n",
    "        )\n",
    "        print(f\"\\nTrain samples: {len(train_df)}\")\n",
    "        print(f\"Validation samples: {len(val_df)}\")\n",
    "        \n",
    "        # Create a dataset of filepaths and labels\n",
    "        train_labels_ds = tf.data.Dataset.from_tensor_slices({\n",
    "            'joint': train_df['joint_label'].values, \n",
    "            'domain': train_df['domain_label'].values\n",
    "        })\n",
    "        train_filepaths_ds = tf.data.Dataset.from_tensor_slices(train_df['filepath'].values)\n",
    "        train_ds = tf.data.Dataset.zip((train_filepaths_ds, train_labels_ds))\n",
    "        \n",
    "        # Build the final, correct training pipeline\n",
    "        train_dataset = train_ds.shuffle(buffer_size=len(train_df))\n",
    "        train_dataset = train_dataset.flat_map(tf_load_and_process)\n",
    "        train_dataset = train_dataset.shuffle(buffer_size=10000)\n",
    "        train_dataset = train_dataset.repeat() # <-- FIX IS HERE\n",
    "        train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        # Build the final validation pipeline\n",
    "        val_labels_ds = tf.data.Dataset.from_tensor_slices({\n",
    "            'joint': val_df['joint_label'].values, \n",
    "            'domain': val_df['domain_label'].values\n",
    "        })\n",
    "        val_filepaths_ds = tf.data.Dataset.from_tensor_slices(val_df['filepath'].values)\n",
    "        val_ds = tf.data.Dataset.zip((val_filepaths_ds, val_labels_ds))\n",
    "        \n",
    "        val_dataset = val_ds.flat_map(tf_load_and_process)\n",
    "        val_dataset = val_dataset.repeat() # <-- AND FIX IS HERE\n",
    "        val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        print(\"\\n Data pipelines created successfully.\")\n",
    "        \n",
    "        model = build_efficient_model() \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "            loss={'joint_output': 'categorical_crossentropy', 'domain_output': 'categorical_crossentropy'},\n",
    "            loss_weights={'joint_output': 1.0, 'domain_output': 0.1},\n",
    "            metrics={'joint_output': 'accuracy', 'domain_output': 'accuracy'}\n",
    "        )\n",
    "        model.summary()\n",
    "        \n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_joint_output_accuracy', mode='max', patience=10, restore_best_weights=True, verbose=1),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6, verbose=1)\n",
    "        ]\n",
    "        \n",
    "        # Calculate steps per epoch\n",
    "        train_steps = len(train_df) * 78 // BATCH_SIZE\n",
    "        val_steps = len(val_df) * 78 // BATCH_SIZE\n",
    "        print(f\"\\nTraining steps per epoch: {train_steps}\")\n",
    "        print(f\"Validation steps per epoch: {val_steps}\")\n",
    "        \n",
    "        # Train the model\n",
    "        print(\"\\n--- Starting Model Training ---\")\n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            validation_data=val_dataset,\n",
    "            epochs=EPOCHS,\n",
    "            steps_per_epoch=train_steps,\n",
    "            validation_steps=val_steps,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(\"\\n Model training complete!\")\n",
    "        \n",
    "        # Final Evaluation\n",
    "        print(\"\\n--- Final Model Evaluation ---\")\n",
    "        results = model.evaluate(val_dataset, steps=val_steps, verbose=0)\n",
    "        \n",
    "        print(\"\\nFinal Performance Metrics:\")\n",
    "        print(f\"Total Loss: {results[0]:.4f}\")\n",
    "        print(f\"Joint Classification Loss: {results[1]:.4f}\")\n",
    "        print(f\"Domain Head Loss: {results[2]:.4f}\")\n",
    "        print(f\"Domain Head Accuracy: {results[3]*100:.2f}%\")\n",
    "        print(f\"Joint Classification Accuracy: {results[4]*100:.2f}%\")\n",
    "        \n",
    "        model.save('efficient_mt_cnn_da_model.h5')\n",
    "        print(\"\\nModel saved as 'efficient_mt_cnn_da_model.h5'\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
